{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be298dc5",
   "metadata": {},
   "source": [
    "## **1. Classification**\n",
    "---\n",
    "### Types of Supervised Learning\n",
    "- With regression, the output is a continuous numerical value\n",
    "- With classification, the output is a category (often a probability between zero and 1)\n",
    "\n",
    "### What is needed for classification\n",
    "- Model data with features that can be quantified\n",
    "- Labels that are known\n",
    "- Methods to measure similarity between classification and desired value\n",
    "\n",
    "### Examples of models used for classification\n",
    "- Logistic Regression: extension of linear regression\n",
    "- K-Nearest neighbours: non-linear simplistic approach to categorise according to similarity of past examples nearest to feature space\n",
    "- Suppor vector machines: linear classifier using the \"kernel trick\"\n",
    "- Neural Networks: combines non-linear and linear intermediate steps to come up with a complex final decision boundary\n",
    "- Decisions tree, random forests, gradient boosting and ensemble models: all build off other classifiers to leverage other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c14f2e",
   "metadata": {},
   "source": [
    "## **2. Mathematics Behind Logistic Regression**\n",
    "---\n",
    "### Introduction\n",
    "- Could predict two possible outcomes using linear regression model, we could fit a straight line to data (<0.5 predict one outcome, >0.5 predict the other)\n",
    "- However the threshold of 0.5 can cause data to be skewed if we use a straight line (ie. larger spread of points and one boundary)\n",
    "- This gives rise to instead of fitting a linear function, we fit a **logistic function**\n",
    "\n",
    "### The Sigmoid Function\n",
    "- Now instead of fitting a simple linear function, we fit the sigmoid of our linear function, defined by:\n",
    "$$\\frac{1}{1+e^{-x}}$$\n",
    "- Here, x is our linear regression function, so:\n",
    "$$y_{\\beta}(x) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_{1}x + \\epsilon)}} = P(x)$$\n",
    "- This remaining function described logistic regression - and is used as a classification algorithm\n",
    "\n",
    "- Key property of the sigmoid is that it clamps all data to the [0-1] range, and it's \"middle\" point is at p=0.5\n",
    "\n",
    "### Probability Link\n",
    "- This function can also be seen as the \"Probability of being in one class over the other\", with P(x) = sigmoid above\n",
    "    - We can get from the above that the \"odds ratio\" (relative probability of two classes) is:\n",
    "$$\\frac{P(x)}{1-P(x)} = e^{(\\beta_0 + \\beta_{1}x + \\epsilon)}$$\n",
    "$$log\\left[\\frac{P(x)}{1-P(x)}\\right] = \\beta_0 + \\beta_{1}x + \\epsilon$$\n",
    "\n",
    "- looking at the log-odds, we can see that a unit increase of $\\beta_0$ or $\\beta_1$, will linearly affect the log-odds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e56af7",
   "metadata": {},
   "source": [
    "## **3. Logistic Regression with Multi-Classes**\n",
    "---\n",
    "### One vs All\n",
    "- To deal with multiple classes with a fucntion (like sigmoid), that predicts binary outcomes, we use one vs all\n",
    "- For each class, we run through the data estimating the binary logistic regression and considering all other classes and \"the other option\"\n",
    "- We then do the same for all other classes, and end up with 3 decision bounaries\n",
    "- The final decision bounaries are decided by the regions where each class has the high probabiliy out of all the classes of occuring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc5787",
   "metadata": {},
   "source": [
    "## **4. Implementing Logistic Regression**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee798dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class containing the classification method\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "# Create an instance of the class\n",
    "lr = LogisticRegression(penalty='l2', C=10.0)\n",
    "\n",
    "# Fit the instance of the class on the training data and then predict the expected values on the test data\n",
    "lr = lr.fit(X_train, y_train)       # type: ignore\n",
    "y_predicted = lr.predict(X_test)    # type: ignore\n",
    "\n",
    "# Can now view the output fitted coefficents\n",
    "lr.coef_\n",
    "\n",
    "# Tune regularisation parameters with cross-validation\n",
    "LogisticRegressionCV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b5b24",
   "metadata": {},
   "source": [
    "- In addition to prediction, we may also want to evaluate the importance of each factor in influencing outcomes (interpretability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958d6ac",
   "metadata": {},
   "source": [
    "## **5. Classification Error Metrics**\n",
    "---\n",
    "- If we have 1% of patients with leukemia and 99% healthy, a simple model could be built that is 99% accurate just by predicting that no one ever has leukemia\n",
    "- So, accuracy is not always the best measurement of performance, and we need others!\n",
    "### Confusion Matrix\n",
    "|                 | Predicted Positive | Predicted Negative |\n",
    "|-----------------|--------------------|--------------------|\n",
    "| **Actual Positive** | True Positive (TP)  | False Negative (FN) ***(Type II Error)***|\n",
    "| **Actual Negative** | False Positive (FP) ***(Type I error)***| True Negative (TN)  |\n",
    "\n",
    "### Accuracy\n",
    "- Accuracy is the most common error measure, and can be calculated as the sum of both correct predictions, over the total number of samples:\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + FN + FP + TN}$$\n",
    "\n",
    "### Recal (sensitivity)\n",
    "- Our ability to identify all the actual positive instances, otherwise known as the capture rate of true instances\n",
    "- Note we can easily acheive 100% recall, just by predicting everything to be positive\n",
    "$$\\text{Recall} = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "### Precision\n",
    "- Precision is where we identify out of all our positive predictions, how many did we get right?\n",
    "$$\\text{Precision} = \\frac{TP}{TP+FP}$$\n",
    "- Note the trade-off between precision and recall\n",
    "\n",
    "### Specificity\n",
    "- Avoiding false alarms, ie. looking at how correctly the actual negative class is predicted (ie. recall for class zero)\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{FP+TN}$$\n",
    "\n",
    "### F1 Score:\n",
    "- Somtimes called the harmonic mean\n",
    "- Tries to optimise the trade-off between recall and precision\n",
    "- This score will more heavily weight if precision or recall are two low\n",
    "$$F1 = 2\\frac{\\text{Precision} * \\text{Recall}}{\\text{Precision}+\\text{Recall}}$$\n",
    "\n",
    "### Roceiver Operating Characteristic Curve (ROC)\n",
    "- Plots True positive Rate (Recall) against the False Positive rate (1-Specificity)\n",
    "- If we have all of our negatives correctly identified, we have a False Positive rate of zero\n",
    "- If we have all of our positives correctly identified, then we have a true positive rate of 1\n",
    "- A point of (FP, TP) is plotted for may different thresholds between 0 and 1 (threshold is the line above/below which we assign a value to a certain class)\n",
    "- A diagonal line from (0, 0) to (1, 1) represents what the outcome would be for random guessing, if our model curve is above this then we are doing better (ideal is perfectly in top right at 1 TP, 0 FP)\n",
    "- If our model is below this diagonal line then we are doing worse than random\n",
    "\n",
    "### ROC-AUC (area under the curve)\n",
    "- Gives a measure of how well we are separating the two classes (just the area under the ROC curve)\n",
    "- For a perfect model, this area would be 1 as we have 1 TP, 0 FP, so a straight line up from zero then a straight line to the right\n",
    "- If we have a ROC-AUC of 0.5, this is essentially as good as random\n",
    "\n",
    "### Presicion-Recall Curve\n",
    "- Plots precision against recall, measuring the trade-off between the two\n",
    "- The area under the curve measures how unbalanced the dataset it\n",
    "\n",
    "\n",
    "### Choosing the right metric\n",
    "1. ROC Curve - better for data with balanced classes\n",
    "2. Precision-Recall Curve - better for data with imbalanced classes\n",
    "\n",
    "The right curve depends on tying results to outcomes (ie. relative cost of False Positive vs False Negative)\n",
    "\n",
    "### Multi-class error metrics\n",
    "$$\\text{Accuracy} = \\frac{TP1 + TP2 +...}{\\text{Total}}$$\n",
    "\n",
    "- For the rest of the metrics, we can still use all of the metrics/curves above in a one vs all way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_score(y_test, y_predicted) # type: ignore\n",
    "\n",
    "# Import other metrics and diagnostic tools\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve,\n",
    "    confusion_matrix,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
