{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be298dc5",
   "metadata": {},
   "source": [
    "## **1. Classification**\n",
    "---\n",
    "### Types of Supervised Learning\n",
    "- With regression, the output is a continuous numerical value\n",
    "- With classification, the output is a category (often a probability between zero and 1)\n",
    "\n",
    "### What is needed for classification\n",
    "- Model data with features that can be quantified\n",
    "- Labels that are known\n",
    "- Methods to measure similarity between classification and desired value\n",
    "\n",
    "### Examples of models used for classification\n",
    "- Logistic Regression: extension of linear regression\n",
    "- K-Nearest neighbours: non-linear simplistic approach to categorise according to similarity of past examples nearest to feature space\n",
    "- Suppor vector machines: linear classifier using the \"kernel trick\"\n",
    "- Neural Networks: combines non-linear and linear intermediate steps to come up with a complex final decision boundary\n",
    "- Decisions tree, random forests, gradient boosting and ensemble models: all build off other classifiers to leverage other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c14f2e",
   "metadata": {},
   "source": [
    "## **2. Mathematics Behind Logistic Regression**\n",
    "---\n",
    "### Introduction\n",
    "- Could predict two possible outcomes using linear regression model, we could fit a straight line to data (<0.5 predict one outcome, >0.5 predict the other)\n",
    "- However the threshold of 0.5 can cause data to be skewed if we use a straight line (ie. larger spread of points and one boundary)\n",
    "- This gives rise to instead of fitting a linear function, we fit a **logistic function**\n",
    "\n",
    "### The Sigmoid Function\n",
    "- Now instead of fitting a simple linear function, we fit the sigmoid of our linear function, defined by:\n",
    "$$\\frac{1}{1+e^{-x}}$$\n",
    "- Here, x is our linear regression function, so:\n",
    "$$y_{\\beta}(x) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_{1}x + \\epsilon)}} = P(x)$$\n",
    "- This remaining function described logistic regression - and is used as a classification algorithm\n",
    "\n",
    "- Key property of the sigmoid is that it clamps all data to the [0-1] range, and it's \"middle\" point is at p=0.5\n",
    "\n",
    "### Probability Link\n",
    "- This function can also be seen as the \"Probability of being in one class over the other\", with P(x) = sigmoid above\n",
    "    - We can get from the above that the \"odds ratio\" (relative probability of two classes) is:\n",
    "$$\\frac{P(x)}{1-P(x)} = e^{(\\beta_0 + \\beta_{1}x + \\epsilon)}$$\n",
    "$$log\\left[\\frac{P(x)}{1-P(x)}\\right] = \\beta_0 + \\beta_{1}x + \\epsilon$$\n",
    "\n",
    "- looking at the log-odds, we can see that a unit increase of $\\beta_0$ or $\\beta_1$, will linearly affect the log-odds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e56af7",
   "metadata": {},
   "source": [
    "## **3. Logistic Regression with Multi-Classes**\n",
    "---\n",
    "### One vs All\n",
    "- To deal with multiple classes with a fucntion (like sigmoid), that predicts binary outcomes, we use one vs all\n",
    "- For each class, we run through the data estimating the binary logistic regression and considering all other classes and \"the other option\"\n",
    "- We then do the same for all other classes, and end up with 3 decision bounaries\n",
    "- The final decision bounaries are decided by the regions where each class has the high probabiliy out of all the classes of occuring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc5787",
   "metadata": {},
   "source": [
    "## **4. Implementing Logistic Regression**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee798dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class containing the classification method\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "# Create an instance of the class\n",
    "lr = LogisticRegression(penalty='l2', C=10.0)\n",
    "\n",
    "# Fit the instance of the class on the training data and then predict the expected values on the test data\n",
    "lr = lr.fit(X_train, y_train)       # type: ignore\n",
    "y_predicted = lr.predict(X_test)    # type: ignore\n",
    "\n",
    "# Can now view the output fitted coefficents\n",
    "lr.coef_\n",
    "\n",
    "# Tune regularisation parameters with cross-validation\n",
    "LogisticRegressionCV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b5b24",
   "metadata": {},
   "source": [
    "- In addition to prediction, we may also want to evaluate the importance of each factor in influencing outcomes (interpretability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958d6ac",
   "metadata": {},
   "source": [
    "## **5. Classification Error Metrics**\n",
    "---\n",
    "- If we have 1% of patients with leukemia and 99% healthy, a simple model could be built that is 99% accurate just by predicting that no one ever has leukemia\n",
    "- So, accuracy is not always the best measurement of performance, and we need others!\n",
    "### Confusion Matrix\n",
    "|                 | Predicted Positive | Predicted Negative |\n",
    "|-----------------|--------------------|--------------------|\n",
    "| **Actual Positive** | True Positive (TP)  | False Negative (FN) ***(Type II Error)***|\n",
    "| **Actual Negative** | False Positive (FP) ***(Type I error)***| True Negative (TN)  |\n",
    "\n",
    "### Accuracy\n",
    "- Accuracy is the most common error measure, and can be calculated as the sum of both correct predictions, over the total number of samples:\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + FN + FP + TN}$$\n",
    "\n",
    "### Recal (sensitivity)\n",
    "- Our ability to identify all the actual positive instances, otherwise known as the capture rate of true instances\n",
    "- Note we can easily acheive 100% recall, just by predicting everything to be positive\n",
    "$$\\text{Recall} = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "### Precision\n",
    "- Precision is where we identify out of all our positive predictions, how many did we get right?\n",
    "$$\\text{Precision} = \\frac{TP}{TP+FP}$$\n",
    "- Note the trade-off between precision and recall\n",
    "\n",
    "### Specificity\n",
    "- Avoiding false alarms, ie. looking at how correctly the actual negative class is predicted (ie. recall for class zero)\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{FP+TN}$$\n",
    "\n",
    "### F1 Score:\n",
    "- Somtimes called the harmonic mean\n",
    "- Tries to optimise the trade-off between recall and precision\n",
    "- This score will more heavily weight if precision or recall are two low\n",
    "$$F1 = 2\\frac{\\text{Precision} * \\text{Recall}}{\\text{Precision}+\\text{Recall}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3ea27",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
