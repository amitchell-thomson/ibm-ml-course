{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322d5cf1",
   "metadata": {},
   "source": [
    "## **1. KNN For Classification**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4cfde",
   "metadata": {},
   "source": [
    "### Classifying Models with KNN\n",
    "- If we have a space with a number of labelled feature vectors (ie. each vector of features corresponds to a certain label) then we can look at the nearest feature vectos and see what their labels are\n",
    "- So, each new observation is compared against K other labelled features with similar observations, and we see what the highest label proportion is, and then that new observation inherits that label\n",
    "\n",
    "### Choosing K\n",
    "- Sometimes there can be an equal number of points with each label for the K nearest neighbours, so we have an unsure classification\n",
    "- To solve this, we can sometimes only choose odd values of K, or we can weight points by their distance to the new observation\n",
    "- K is just another hyperparameter we can tune to get the best model for prediction\n",
    "\n",
    "### What is needed to select a KNN Model?\n",
    "- We need a correct value of K (ie. how many nearest neighbours)\n",
    "- We also need a way to measure the closeness of neighbours (number of ways to do this)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83764b22",
   "metadata": {},
   "source": [
    "## **2. Decision Boundary for KNN**\n",
    "---\n",
    "- Shows the regions in space where a point would be classified to a certain region if it landed there\n",
    "- The decision boundary depends on our value of K\n",
    "    - Choosing K=1 will just give you a model that predicts based on whichever point your observation vector is closest too (Underfit)\n",
    "    - Choosing K=(all points) will give you a model that predicts the majority class for everything, as every time all points are considered (Overfit)\n",
    "\n",
    "### Choosing the right value for K\n",
    "- KNN does not provide a \"correct\" value for K, and the right value depends on what error metric is most important to you\n",
    "- A common approach is to plot the error rate as a function of K, and find the so called \"elbow point\" (essentially the minima of the curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c07c1b",
   "metadata": {},
   "source": [
    "## **3. Measurement of Distance in KNN**\n",
    "---\n",
    "### Euclidean Distance\n",
    "- The actual physical distance between two points\n",
    "- Take the square-root of the sum of the squares of the distance in each axes (square root of square of differences between each feature)\n",
    "\n",
    "### Manhattan Distance L1 Norm\n",
    "- The sum of the absolute distances between each feature\n",
    "\n",
    "### Scale for Distance Measurement\n",
    "- Since our model is so dependent on distance between features, the scaling of features matters a lot for feature importance and classification\n",
    "- So we need to scale our features to the same range/mean/variance:\n",
    "    - Min/max scaling: (fit data into 0-1 range)\n",
    "    - Standard scaling: (take the z-score of the data)\n",
    "\n",
    "### Prediction of Multiple classes\n",
    "- Very simple for KNN, as our decision boundaries just fit around new classes (we just look at which class is the majority nearest)\n",
    "- If we have N classes, we may choose to make K = aN + 1 so that one class always has majority vote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c018000",
   "metadata": {},
   "source": [
    "## **4. Regression with KNN**\n",
    "---\n",
    "- Also incredibly simple, where our predicted value is just the mean value of all of its K neighbours\n",
    "\n",
    "- We can also choose to weight each neighbour by its relative distance to the point\n",
    "    - For large K, we basically always predict that new observations have value equal to the mean of the data\n",
    "    - For medium K, KNN acts as a smoothing function between points\n",
    "    - For K=1, KNN just predicts the output as the value of the nearest neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d3400c",
   "metadata": {},
   "source": [
    "## **5. Pros and Cons of KNN**\n",
    "---\n",
    "### Pros\n",
    "- Simple to implement (no estimation required)\n",
    "- Adapts well as new data is introduced\n",
    "- Easy to interpret, very useful when leveraging machine learning for business\n",
    "\n",
    "### Cons\n",
    "- Slow to predict because there are many distance calculations\n",
    "- Does not generate insight into the data generating process (no model)\n",
    "- Can require a lot of memory if dataset is large (or as it grows), as the model needs to store all the values in the training set every time it fits the model\n",
    "- When there are many features, KNN accuracy can break down due to more dimensions meaning more distances ie. larger error\n",
    "\n",
    "### KNN vs Linear Regression\n",
    "- Linear regression:\n",
    "    - Fitting involves minimising cost function (slow)\n",
    "    - Model has few parameters (memory efficient)\n",
    "    - Prediction involves calculation (fast)\n",
    "\n",
    "- KNN\n",
    "    - Fitting involves storing training data (fast)\n",
    "    - Model has many parameters (memory intensive)\n",
    "    - Prediction involves finding closesnt neighbors (slow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a52a14",
   "metadata": {},
   "source": [
    "## **6. Implementing KNN**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9017c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the class containing the classification method\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create an instance of the KNN classifier\n",
    "KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit the model to the training data\n",
    "KNN = KNN.fit(X_train, y_train) # type: ignore\n",
    "\n",
    "# Predict the response for the test dataset\n",
    "y_predict = KNN.predict(X_test) # type: ignore\n",
    "\n",
    "# Regression can also be done with:\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
